{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "814a9291",
   "metadata": {},
   "source": [
    "# Build anomaly 2060-2099 (2160-2199) wrt 1979-2018 onto JRA grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de62f097",
   "metadata": {},
   "source": [
    "## Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab439bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import copy as copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd629649",
   "metadata": {},
   "source": [
    "## Core functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d658082",
   "metadata": {},
   "source": [
    "### compute 3h anomaly (with and without leap year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3034ae21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_1m_to_3h_ano(ds_month_ano):\n",
    "    \"\"\"\n",
    "    Purpose: compute 3h anomaly based on a monthly file\n",
    "    \n",
    "    Args:\n",
    "        ds_month_ano : monthly anomaly [xarray dataset]\n",
    "        \n",
    "    Return: \n",
    "        ds_3h_ano : 3hourly anomaly [xarray dataset]\n",
    "    \"\"\"\n",
    "    \n",
    "    # Concat last 3 months at the beginning and first 3 month at the end\n",
    "    # It is needed for a coorect interpolation as the end (31st december should match the 1st January minus 1 day)\n",
    "    # 3 month as depending of the stencil of the interpolation scheme it could be needed\n",
    "    m0=ds_month_ano.isel(month=[9,10,11])\n",
    "    m0['month']=m0['month']-pd.Timedelta(days=365)\n",
    "    m13=ds_month_ano.isel(month=[0,1,2])\n",
    "    m13['month']=m13['month']+pd.Timedelta(days=365)\n",
    "    ds_3h_ano=xr.concat([m0,ds_month_ano,m13],dim='month')\n",
    "\n",
    "    # resampling with quadratic interpolation\n",
    "    ds_3h_ano=ds_3h_ano.resample(month=\"3H\").interpolate(\"quadratic\")\n",
    "\n",
    "    # select data only between 1st January to 31st december\n",
    "    ds_3h_ano=ds_3h_ano.sel(month=slice('{:d}-01-01'.format(idateout), '{:d}-12-31'.format(idateout)))\n",
    "\n",
    "    return ds_3h_ano"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ff24de",
   "metadata": {},
   "source": [
    "### Add overlap to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f99b917",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_EW_overlap(ds):\n",
    "    \"\"\"\n",
    "    Purpose: add 3 overlap cell on East and West boundary (useful before interpolation)\n",
    "    \n",
    "    Args:\n",
    "        ds : input dataset [xarray dataset]\n",
    "        \n",
    "    Return: \n",
    "        ds : output dataset with the 3 most Eastern cells added at the West bnd \n",
    "             and the 3 most Western cells added at the East bnd [xarray dataset]\n",
    "    \"\"\"\n",
    "    \n",
    "    # Concat last 3 months at the beginning and first 3 month at the end\n",
    "    # It is needed for a coorect interpolation as the end (31st december should match the 1st January minus 1 day)\n",
    "    # 3 month as depending of the stencil of the interpolation scheme it could be needed\n",
    "    lon0=ds.isel(longitude=slice(-3,None,1))\n",
    "    lon0['longitude'] = lon0['longitude']-360.\n",
    "    \n",
    "    lon360=ds.isel(longitude=slice(0,3,1))\n",
    "    lon360['longitude'] = lon360['longitude']+360.\n",
    "    \n",
    "    return xr.concat([lon0,ds_in,lon360],dim='longitude')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e46a8a4",
   "metadata": {},
   "source": [
    "### Define new attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4eefe9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_attributes_anomaly(cvar,dattrs,ds):\n",
    "    \"\"\"\n",
    "    Purpose: modify variable reference attributes\n",
    "    \n",
    "    Args:\n",
    "        dattrs : old attributes [dict]\n",
    "        cvar   : variable name used to extract min and max [ string ]\n",
    "        ds     : dataset from where min and max are extracted [ xarray dataset ]\n",
    "        \n",
    "    Return: \n",
    "        dattrs : new attributes [dict]\n",
    "    \"\"\"\n",
    "    \n",
    "    # get rid of non sense attributes\n",
    "    if 'time' in list(dattrs.keys()):\n",
    "        del dattrs[\"time\"]\n",
    "    if 'date' in list(dattrs.keys()):\n",
    "        del dattrs[\"date\"]\n",
    "        \n",
    "    # modify existing attributes\n",
    "    dattrs[\"name\"]      = dattrs[\"name\"]+'_anomaly'\n",
    "    dattrs[\"title\"]     = dattrs[\"title\"]+' anomaly'\n",
    "    dattrs[\"long_name\"] = dattrs[\"long_name\"]+' anomaly'\n",
    "    \n",
    "    # compute new valid range\n",
    "    dattrs[\"valid_min\"] = ds[cvar].min().values\n",
    "    dattrs[\"valid_max\"] = ds[cvar].max().values\n",
    "    \n",
    "    return dattrs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedb94fd",
   "metadata": {},
   "source": [
    "## Sanity check function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0f1997",
   "metadata": {},
   "source": [
    "### Display global mean anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ace7cbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_annual_global_mean_ano(ds_ano,cvar):\n",
    "    \"\"\"\n",
    "    Purpose: print annual global mean\n",
    "    \n",
    "    Args:\n",
    "        ds_ano : dataset used to extract annual global mean [xarray dataset]\n",
    "        cvar   : variable name used to extract annual global mean [ string ]\n",
    "        \n",
    "    Warning : grid is assumed to be regular and to scale with latitude\n",
    "    \"\"\"\n",
    "    \n",
    "    # compute annual anomaly\n",
    "    zds=ds_ano[cvar].mean(dim='month')\n",
    "    \n",
    "    # compute weights\n",
    "    weights = np.cos(np.deg2rad(zds.latitude,dtype=np.float64))\n",
    "    \n",
    "    # compute weighted xarray\n",
    "    weights.name = \"weights\"\n",
    "    zds = zds.weighted(weights)\n",
    "    \n",
    "    # display anomaly\n",
    "    print('        annual mean anomaly is : ',zds.mean((\"longitude\", \"latitude\")).values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28db7b55",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35ae097c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data dir path\n",
    "cdir='DATA_in/HadCM3'\n",
    "\n",
    "# JRA grid\n",
    "cfout_grid='DATAin/JRA/JRA_grid.nc'\n",
    "\n",
    "# list of file variables to process and the corresponding infile variables\n",
    "fvarlst=['Q_1_5M','LW_TOTAL_DOWNWARD_SURFACE','P_SURF','SW_TOTAL_DOWNWARD_SURFACE','TOTAL_PRECIP','T_AIR_1_5M','U_10M','V_10M']\n",
    "cvarlst =['q'     ,'ilr'                      ,'p'     ,'field203'                 ,'precip'      ,'temp'      ,'u'    ,    'v']\n",
    "\n",
    "# reference period\n",
    "yrefs=1979; yrefe=2019\n",
    "\n",
    "# trg period\n",
    "period='2060-2100' #2160-2200\n",
    "if period == '2060-2100':\n",
    "    # input file name extension where the data are\n",
    "    cftrg_ext='2000_2099'\n",
    "    # start and end of the period used\n",
    "    ytrgs=2060; ytrge=2100\n",
    "elif period == '2160-2200':\n",
    "    cftrg_ext='2100_2199'\n",
    "    ytrgs=2160; ytrge=2200\n",
    "else:\n",
    "    raise NameError('period not found')\n",
    "\n",
    "cfext = '{:d}{:d}-{:d}{:d}'.format(ytrgs,ytrge,yrefs,yrefe)\n",
    "\n",
    "idateout = 1951 # use a non leap year (0001 is out of bound for date_range pandas function )\n",
    "\n",
    "# slice definition\n",
    "slc_ref=slice('{:d}-01-01'.format(yrefs), '{:d}-01-01'.format(yrefe))\n",
    "slc_trg=slice('{:d}-01-01'.format(ytrgs), '{:d}-01-01'.format(ytrge))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27af006e",
   "metadata": {},
   "source": [
    "## Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b409a51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute 3h anomaly between period 2100-2060 and 2019-1979 for data in DATA_in/HadCM3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Compute 3h anomaly between period {:d}-{:d} and {:d}-{:d} for data in {}'.format(ytrge,ytrgs,yrefe,yrefs,cdir))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072362ee",
   "metadata": {},
   "source": [
    "### Compute 3 hourly anomaly from monthly data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a2c0fe",
   "metadata": {},
   "source": [
    "For each variable, the steps are :\n",
    " - open data\n",
    " - compute climato\n",
    " - compute anomaly\n",
    " - sample to 3h (quadratic interpolation)\n",
    " - write data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88d266a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing of Q_1_5M (q) in progress ...\n",
      "    load reference data ...\n",
      "        file : DATA_in/HadCM3/C20_Q_1_5M_1900_1999.NC\n",
      "        file : DATA_in/HadCM3/A1B_Q_1_5M_2000_2099.NC\n",
      "        combine dataset DATA_in/HadCM3/C20_Q_1_5M_1900_1999.NC and DATA_in/HadCM3/A1B_Q_1_5M_2000_2099.NC\n",
      "    load period data ...\n",
      "        file : DATA_in/HadCM3/A1B_Q_1_5M_2000_2099.NC\n",
      "    build monthly climato ...\n",
      "    compute anomaly ...\n",
      "        annual mean anomaly is :  0.0014433373913731967\n",
      "    write global att ...\n",
      "    write data ...\n",
      "        file : A1B_Q_1_5M_3h_ano_20602100-19792019.NC ...\n",
      "\n",
      "\n",
      "Processing of LW_TOTAL_DOWNWARD_SURFACE (ilr) in progress ...\n",
      "    load reference data ...\n",
      "        file : DATA_in/HadCM3/C20_LW_TOTAL_DOWNWARD_SURFACE_1900_1999.NC\n",
      "        file : DATA_in/HadCM3/A1B_LW_TOTAL_DOWNWARD_SURFACE_2000_2099.NC\n",
      "        combine dataset DATA_in/HadCM3/C20_LW_TOTAL_DOWNWARD_SURFACE_1900_1999.NC and DATA_in/HadCM3/A1B_LW_TOTAL_DOWNWARD_SURFACE_2000_2099.NC\n",
      "    load period data ...\n",
      "        file : DATA_in/HadCM3/A1B_LW_TOTAL_DOWNWARD_SURFACE_2000_2099.NC\n",
      "    build monthly climato ...\n",
      "    compute anomaly ...\n",
      "        annual mean anomaly is :  17.66566121988594\n",
      "    write global att ...\n",
      "    write data ...\n",
      "        file : A1B_LW_TOTAL_DOWNWARD_SURFACE_3h_ano_20602100-19792019.NC ...\n",
      "\n",
      "\n",
      "Processing of P_SURF (p) in progress ...\n",
      "    load reference data ...\n",
      "        file : DATA_in/HadCM3/C20_P_SURF_1900_1999.NC\n",
      "        file : DATA_in/HadCM3/A1B_P_SURF_2000_2099.NC\n",
      "        combine dataset DATA_in/HadCM3/C20_P_SURF_1900_1999.NC and DATA_in/HadCM3/A1B_P_SURF_2000_2099.NC\n",
      "    load period data ...\n",
      "        file : DATA_in/HadCM3/A1B_P_SURF_2000_2099.NC\n",
      "    build monthly climato ...\n",
      "    compute anomaly ...\n",
      "        annual mean anomaly is :  -0.005891112411687506\n",
      "    write global att ...\n",
      "    write data ...\n",
      "        file : A1B_P_SURF_3h_ano_20602100-19792019.NC ...\n",
      "\n",
      "\n",
      "Processing of SW_TOTAL_DOWNWARD_SURFACE (field203) in progress ...\n",
      "    load reference data ...\n",
      "        file : DATA_in/HadCM3/C20_SW_TOTAL_DOWNWARD_SURFACE_1900_1999.NC\n",
      "        file : DATA_in/HadCM3/A1B_SW_TOTAL_DOWNWARD_SURFACE_2000_2099.NC\n",
      "        combine dataset DATA_in/HadCM3/C20_SW_TOTAL_DOWNWARD_SURFACE_1900_1999.NC and DATA_in/HadCM3/A1B_SW_TOTAL_DOWNWARD_SURFACE_2000_2099.NC\n",
      "    load period data ...\n",
      "        file : DATA_in/HadCM3/A1B_SW_TOTAL_DOWNWARD_SURFACE_2000_2099.NC\n",
      "    build monthly climato ...\n",
      "    compute anomaly ...\n",
      "        annual mean anomaly is :  -1.115107735396427\n",
      "    write global att ...\n",
      "    write data ...\n",
      "        file : A1B_SW_TOTAL_DOWNWARD_SURFACE_3h_ano_20602100-19792019.NC ...\n",
      "\n",
      "\n",
      "Processing of TOTAL_PRECIP (precip) in progress ...\n",
      "    load reference data ...\n",
      "        file : DATA_in/HadCM3/C20_TOTAL_PRECIP_1900_1999.NC\n",
      "        file : DATA_in/HadCM3/A1B_TOTAL_PRECIP_2000_2099.NC\n",
      "        combine dataset DATA_in/HadCM3/C20_TOTAL_PRECIP_1900_1999.NC and DATA_in/HadCM3/A1B_TOTAL_PRECIP_2000_2099.NC\n",
      "    load period data ...\n",
      "        file : DATA_in/HadCM3/A1B_TOTAL_PRECIP_2000_2099.NC\n",
      "    build monthly climato ...\n",
      "    compute anomaly ...\n",
      "        annual mean anomaly is :  8.601794393411911e-07\n",
      "    write global att ...\n",
      "    write data ...\n",
      "        file : A1B_TOTAL_PRECIP_3h_ano_20602100-19792019.NC ...\n",
      "\n",
      "\n",
      "Processing of T_AIR_1_5M (temp) in progress ...\n",
      "    load reference data ...\n",
      "        file : DATA_in/HadCM3/C20_T_AIR_1_5M_1900_1999.NC\n",
      "        file : DATA_in/HadCM3/A1B_T_AIR_1_5M_2000_2099.NC\n",
      "        combine dataset DATA_in/HadCM3/C20_T_AIR_1_5M_1900_1999.NC and DATA_in/HadCM3/A1B_T_AIR_1_5M_2000_2099.NC\n",
      "    load period data ...\n",
      "        file : DATA_in/HadCM3/A1B_T_AIR_1_5M_2000_2099.NC\n",
      "    build monthly climato ...\n",
      "    compute anomaly ...\n",
      "        annual mean anomaly is :  2.677373099994075\n",
      "    write global att ...\n",
      "    write data ...\n",
      "        file : A1B_T_AIR_1_5M_3h_ano_20602100-19792019.NC ...\n",
      "\n",
      "\n",
      "Processing of U_10M (u) in progress ...\n",
      "    load reference data ...\n",
      "        file : DATA_in/HadCM3/C20_U_10M_1900_1999.NC\n",
      "        file : DATA_in/HadCM3/A1B_U_10M_2000_2099.NC\n",
      "        combine dataset DATA_in/HadCM3/C20_U_10M_1900_1999.NC and DATA_in/HadCM3/A1B_U_10M_2000_2099.NC\n",
      "    load period data ...\n",
      "        file : DATA_in/HadCM3/A1B_U_10M_2000_2099.NC\n",
      "    build monthly climato ...\n",
      "    compute anomaly ...\n",
      "        annual mean anomaly is :  0.0568460783281017\n",
      "    write global att ...\n",
      "    write data ...\n",
      "        file : A1B_U_10M_3h_ano_20602100-19792019.NC ...\n",
      "\n",
      "\n",
      "Processing of V_10M (v) in progress ...\n",
      "    load reference data ...\n",
      "        file : DATA_in/HadCM3/C20_V_10M_1900_1999.NC\n",
      "        file : DATA_in/HadCM3/A1B_V_10M_2000_2099.NC\n",
      "        combine dataset DATA_in/HadCM3/C20_V_10M_1900_1999.NC and DATA_in/HadCM3/A1B_V_10M_2000_2099.NC\n",
      "    load period data ...\n",
      "        file : DATA_in/HadCM3/A1B_V_10M_2000_2099.NC\n",
      "    build monthly climato ...\n",
      "    compute anomaly ...\n",
      "        annual mean anomaly is :  0.04272798046018452\n",
      "    write global att ...\n",
      "    write data ...\n",
      "        file : A1B_V_10M_3h_ano_20602100-19792019.NC ...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for each variable\n",
    "for ivar,fvar in enumerate(fvarlst):\n",
    "    cvar=cvarlst[ivar]\n",
    "    cdvar='d'+cvar\n",
    "    \n",
    "    print('Processing of {} ({}) in progress ...'.format(fvar,cvar))\n",
    "    \n",
    "    # load data reference\n",
    "    print('    load reference data ...')\n",
    "    cfrefin1='{}/C20_{}_1900_1999.NC'.format(cdir,fvar)\n",
    "    cfrefin2='{}/A1B_{}_2000_2099.NC'.format(cdir,fvar)\n",
    "    print('        file : {}'.format(cfrefin1))\n",
    "    ds_ref1 = xr.open_dataset(cfrefin1)\n",
    "    print('        file : {}'.format(cfrefin2))\n",
    "    ds_ref2 = xr.open_dataset(cfrefin2)\n",
    "    print('        combine dataset {} and {}'.format(cfrefin1,cfrefin2))\n",
    "    ds_ref  = xr.combine_by_coords([ds_ref1,ds_ref2],combine_attrs='drop_conflicts').squeeze()\n",
    "    \n",
    "    # load data trg\n",
    "    print('    load period data ...')\n",
    "    cftrgin1='{}/A1B_{}_{}.NC'.format(cdir,fvar,cftrg_ext)\n",
    "    print('        file : {}'.format(cftrgin1))\n",
    "    ds_trg = xr.open_dataset(cftrgin1).squeeze()\n",
    "    \n",
    "    # drop useless variables\n",
    "    if 'ht' in list(ds_ref.coords.keys()):\n",
    "        ds_ref=ds_ref.drop('ht');\n",
    "        ds_trg=ds_trg.drop('ht')\n",
    "        \n",
    "    # compute climato\n",
    "    print('    build monthly climato ...')\n",
    "    ds_ref_clim=ds_ref.sel(t=slc_ref).groupby('t.month').mean(dim='t')\n",
    "    ds_trg_clim=ds_trg.sel(t=slc_trg).groupby('t.month').mean(dim='t')\n",
    "\n",
    "    # compute anomaly\n",
    "    print('    compute anomaly ...')\n",
    "    ds_trg_ano = ds_trg_clim - ds_ref_clim\n",
    "\n",
    "    # print global annual mean anomaly\n",
    "    print_annual_global_mean_ano(ds_trg_ano,cvar)\n",
    "    \n",
    "    # update month dimension (use idateout as dummy date 0001 was not possible (out of bound))\n",
    "    ds_trg_ano['month']=pd.date_range(\"{:d}/01/15\".format(idateout),periods=12,freq=pd.DateOffset(months=1))\n",
    "    \n",
    "    # compute 3h anomaly (with and without leap year)\n",
    "    ds_3h_ano=compute_1m_to_3h_ano(ds_trg_ano)\n",
    "    \n",
    "    # rename time dim\n",
    "    ds_3h_ano=ds_3h_ano.rename({'month': 'time'})\n",
    "    \n",
    "    # rename variable\n",
    "    ds_3h_ano=ds_3h_ano.rename({cvar:cdvar})\n",
    "    \n",
    "    # add variable attributes\n",
    "    ds_3h_ano['d'+cvar].attrs = modify_attributes_anomaly(cdvar,ds_ref[cvar].attrs,ds_3h_ano)\n",
    "    \n",
    "    # add global attributes\n",
    "    print('    write global att ...')\n",
    "    ds_3h_ano.attrs =  { 'Description' : 'The above anomaly file is the differences between a specific period and a reference period',\n",
    "                         'Specific Period' : '{:d} - {:d}'.format(ytrge, ytrgs),\n",
    "                         'Reference period' : '{:d} - {:d}'.format(yrefe, yrefs),\n",
    "                         'Model' : 'HadCM3',\n",
    "                         'Scenario'   : 'A1B',\n",
    "                         'Contact':'P. Mathiot (IGE)',\n",
    "                         'Creation date':'{}'.format(datetime.now()),\n",
    "                        }\n",
    "    \n",
    "    # write data\n",
    "    print('    write data ...')\n",
    "    cfout=\"A1B_{}_3h_ano_{}.NC\".format(fvar,cfext)\n",
    "    print('        file : {} ...'.format(cfout))\n",
    "    ds_3h_ano.to_netcdf('DATA_out/'+cfout,unlimited_dims='time',encoding={'time' : {'dtype': 'float64'}})\n",
    "    \n",
    "    print('')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19099242",
   "metadata": {},
   "source": [
    "## Interpolate onto JRA grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde09e9a",
   "metadata": {},
   "source": [
    "### Load JRA grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5671abb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfJRA = 'DATA_in/JRA/JRA55_grid.nc'\n",
    "ds_JRA = xr.open_dataset(cfJRA)\n",
    "lat_JRA=ds_JRA['lat']\n",
    "lon_JRA=ds_JRA['lon']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdf8635",
   "metadata": {},
   "source": [
    "### Interpolate HadCM3 data to JRA grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2bf42d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    load HadCM3 data ...\n",
      "        file : ./A1B_Q_1_5M_3h_ano_20602100-19792019.NC\n",
      "    extend input data E and W by 3 data points ...\n",
      "    interpolate data onto JRA grid ...\n",
      "        method = cubic\n",
      "    clean output data set ...\n",
      "        update valid range attributes ...\n",
      "        drop useless coordinates ...\n",
      "        update global attributes ...\n",
      "    write data interpolated onto JRA grid ...\n",
      "        file :  A1B_Q_1_5M_3h_ano_20602100-19792019_on_JRA_grid.NC\n",
      "\n",
      "\n",
      "    load HadCM3 data ...\n",
      "        file : ./A1B_LW_TOTAL_DOWNWARD_SURFACE_3h_ano_20602100-19792019.NC\n",
      "    extend input data E and W by 3 data points ...\n",
      "    interpolate data onto JRA grid ...\n",
      "        method = cubic\n",
      "    clean output data set ...\n",
      "        update valid range attributes ...\n",
      "        drop useless coordinates ...\n",
      "        update global attributes ...\n",
      "    write data interpolated onto JRA grid ...\n",
      "        file :  A1B_LW_TOTAL_DOWNWARD_SURFACE_3h_ano_20602100-19792019_on_JRA_grid.NC\n",
      "\n",
      "\n",
      "    load HadCM3 data ...\n",
      "        file : ./A1B_P_SURF_3h_ano_20602100-19792019.NC\n",
      "    extend input data E and W by 3 data points ...\n",
      "    interpolate data onto JRA grid ...\n",
      "        method = cubic\n",
      "    clean output data set ...\n",
      "        update valid range attributes ...\n",
      "        drop useless coordinates ...\n",
      "        update global attributes ...\n",
      "    write data interpolated onto JRA grid ...\n",
      "        file :  A1B_P_SURF_3h_ano_20602100-19792019_on_JRA_grid.NC\n",
      "\n",
      "\n",
      "    load HadCM3 data ...\n",
      "        file : ./A1B_SW_TOTAL_DOWNWARD_SURFACE_3h_ano_20602100-19792019.NC\n",
      "    extend input data E and W by 3 data points ...\n",
      "    interpolate data onto JRA grid ...\n",
      "        method = cubic\n",
      "    clean output data set ...\n",
      "        update valid range attributes ...\n",
      "        drop useless coordinates ...\n",
      "        update global attributes ...\n",
      "    write data interpolated onto JRA grid ...\n",
      "        file :  A1B_SW_TOTAL_DOWNWARD_SURFACE_3h_ano_20602100-19792019_on_JRA_grid.NC\n",
      "\n",
      "\n",
      "    load HadCM3 data ...\n",
      "        file : ./A1B_TOTAL_PRECIP_3h_ano_20602100-19792019.NC\n",
      "    extend input data E and W by 3 data points ...\n",
      "    interpolate data onto JRA grid ...\n",
      "        method = cubic\n",
      "    clean output data set ...\n",
      "        update valid range attributes ...\n",
      "        drop useless coordinates ...\n",
      "        update global attributes ...\n",
      "    write data interpolated onto JRA grid ...\n",
      "        file :  A1B_TOTAL_PRECIP_3h_ano_20602100-19792019_on_JRA_grid.NC\n",
      "\n",
      "\n",
      "    load HadCM3 data ...\n",
      "        file : ./A1B_T_AIR_1_5M_3h_ano_20602100-19792019.NC\n",
      "    extend input data E and W by 3 data points ...\n",
      "    interpolate data onto JRA grid ...\n",
      "        method = cubic\n",
      "    clean output data set ...\n",
      "        update valid range attributes ...\n",
      "        drop useless coordinates ...\n",
      "        update global attributes ...\n",
      "    write data interpolated onto JRA grid ...\n",
      "        file :  A1B_T_AIR_1_5M_3h_ano_20602100-19792019_on_JRA_grid.NC\n",
      "\n",
      "\n",
      "    load HadCM3 data ...\n",
      "        file : ./A1B_U_10M_3h_ano_20602100-19792019.NC\n",
      "    extend input data E and W by 3 data points ...\n",
      "    interpolate data onto JRA grid ...\n",
      "        method = cubic\n",
      "    clean output data set ...\n",
      "        update valid range attributes ...\n",
      "        drop useless coordinates ...\n",
      "        update global attributes ...\n",
      "    write data interpolated onto JRA grid ...\n",
      "        file :  A1B_U_10M_3h_ano_20602100-19792019_on_JRA_grid.NC\n",
      "\n",
      "\n",
      "    load HadCM3 data ...\n",
      "        file : ./A1B_V_10M_3h_ano_20602100-19792019.NC\n",
      "    extend input data E and W by 3 data points ...\n",
      "    interpolate data onto JRA grid ...\n",
      "        method = cubic\n",
      "    clean output data set ...\n",
      "        update valid range attributes ...\n",
      "        drop useless coordinates ...\n",
      "        update global attributes ...\n",
      "    write data interpolated onto JRA grid ...\n",
      "        file :  A1B_V_10M_3h_ano_20602100-19792019_on_JRA_grid.NC\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for ivar,fvar in enumerate(fvarlst):\n",
    "    cvar=cvarlst[ivar]\n",
    "    cdvar='d'+cvar\n",
    "    \n",
    "    # load data in\n",
    "    print('    load HadCM3 data ...')\n",
    "    cfin='./A1B_{}_3h_ano_{}.NC'.format(fvar,cfext)\n",
    "    print('        file : {}'.format(cfin))\n",
    "    ds_in = xr.open_dataset('./DATA_out/'+cfin)\n",
    "    \n",
    "    print('    extend input data E and W by 3 data points ...')\n",
    "    ds_in = add_EW_overlap(ds_in)\n",
    "    \n",
    "    print('    interpolate data onto JRA grid ...')\n",
    "    print('        method = cubic')\n",
    "    ds_out = ds_in.interp(latitude=lat_JRA, longitude=lon_JRA, method='cubic')\n",
    "    \n",
    "    print('    clean output data set ...')\n",
    "    print('        update valid range attributes ...')\n",
    "    ds_out[cdvar].attrs['valid_min']=ds_out[cdvar].min().values\n",
    "    ds_out[cdvar].attrs['valid_max']=ds_out[cdvar].max().values\n",
    "    \n",
    "    print('        drop useless coordinates ...')\n",
    "    ds_out = ds_out.drop(['longitude','latitude'])\n",
    "    \n",
    "    print('        update global attributes ...')\n",
    "    ds_out.attrs['Description']=ds_out.attrs['Description']+' interpolated onto JRA55 grid with a cubic method'\n",
    "    ds_out.attrs['Grid']='JRA55 grid'\n",
    "    ds_out.attrs['Creation date'] = '{}'.format(datetime.now())\n",
    "    \n",
    "    print('    write data interpolated onto JRA grid ...')\n",
    "    cfout='A1B_{}_3h_ano_{}_on_JRA_grid.NC'.format(fvar,cfext)\n",
    "    print('        file : ',cfout)\n",
    "    ds_out.to_netcdf('./DATA_out/'+cfout)\n",
    "    \n",
    "    print()\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "A1B_perturbation",
   "language": "python",
   "name": "a1b_perturbation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
